# llm:
#   provider: ollama
#   model_name: llama3.2
#   temperature: 0
#   max_tokens: 2048
llm:
  provider: openai
  model_name: gpt-4o-mini
  temperature: 0
  max_tokens: 2048

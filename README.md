# ARDI - AI Agent for News Media Data Exploration

> **Master's Thesis Project**  
> Free University of Bozen-Bolzano (UNIBZ)  
> Master's Degree in Data Science  
> Developing an AI Agent for Data Exploration in the News Media Industry

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Project Context](#project-context)
- [Architecture](#architecture)
- [Key Features](#key-features)
- [Technology Stack](#technology-stack)
- [Project Structure](#project-structure)
- [Installation](#installation)
- [Configuration](#configuration)
- [Usage](#usage)
- [API Endpoints](#api-endpoints)
- [Agent Workflow](#agent-workflow)
- [Data Sources](#data-sources)
- [Evaluation](#evaluation)
- [Contributing](#contributing)
- [License](#license)

---

## ğŸ¯ Overview

**ARDI** (Analytical Reasoning for Data Insights) is an intelligent AI agent designed to assist news editors, analysts, and content strategists in understanding and interpreting reader behavior with digital news content. The system leverages Large Language Models (LLMs) to provide natural language interfaces for complex data analytics tasks in the news media industry.

This project demonstrates how AI agents can bridge the gap between complex data analytics and non-technical users, enabling news organizations to make data-driven editorial decisions through conversational interfaces.

---

## ğŸŒ Project Context

This thesis addresses a critical challenge in the news media industry: **making audience analytics accessible to editorial teams**. Traditional analytics tools require technical expertise, creating a barrier between data insights and editorial decision-making.

### Business Problem

News organizations collect vast amounts of reader interaction data but struggle to:
- Understand what topics attract specific user segments
- Analyze when and how readers consume news
- Identify which articles or topics drive engagement
- Make these insights accessible to non-technical editorial staff

### Solution

ARDI provides a conversational AI interface that:
- Translates natural language questions into analytical workflows
- Executes multi-step data analysis tasks autonomously
- Generates human-readable insights from complex data
- Operates within the context of a German regional news organization

---

## ğŸ—ï¸ Architecture

ARDI implements an **agentic AI architecture** with the following components:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        User Interface                        â”‚
â”‚                    (FastAPI REST API)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ARDI Agent Core                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚   Planner    â”‚â†’ â”‚   Executor   â”‚â†’ â”‚  Responder   â”‚      â”‚
â”‚  â”‚  (LangChain) â”‚  â”‚  (LangGraph) â”‚  â”‚    (LLM)     â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Analytical Tools Layer                    â”‚
â”‚  â€¢ User Segmentation    â€¢ News Topics    â€¢ Article Data     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Data Sources (Pickle/JSON)                â”‚
â”‚  â€¢ user_segments_viz.pkl  â€¢ news_topics.pkl                 â”‚
â”‚  â€¢ news_viz2.json                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Components

1. **Task Planning Module** (`planner.py`)
   - Converts user questions into structured analytical plans
   - Validates task dependencies and tool availability
   - Uses LLM with structured output for plan generation

2. **Task Execution Module** (`executor.py`)
   - Executes analytical tasks in dependency order
   - Manages data flow between tasks
   - Supports dynamic plan updates based on intermediate results

3. **Response Generation Module** (`responder.py`)
   - Synthesizes analytical results into natural language
   - Provides context-aware explanations
   - Formats insights for editorial decision-making

4. **Workflow Orchestration** (`workflow.py`)
   - Implements state machine using LangGraph
   - Manages execution flow and error handling
   - Provides checkpointing for conversation continuity

---

## âœ¨ Key Features

### ğŸ¤– Intelligent Query Planning
- Automatically decomposes complex questions into analytical steps
- Identifies required data sources and tools
- Validates plan feasibility before execution

### ğŸ”„ Multi-Step Execution
- Executes tasks with dependency resolution
- Chains multiple analytical operations
- Supports dynamic replanning based on intermediate results

### ğŸ“Š Comprehensive Analytics Tools
- **15+ specialized analytical functions** covering:
  - User segment analysis (demographics, behavior, engagement)
  - Topic modeling and transitions
  - Temporal activity patterns
  - Regional consumption analysis
  - Article performance metrics

### ğŸ’¬ Natural Language Interface
- Conversational query input
- Context-aware responses
- Journalistic tone adapted for newsroom environments

### ğŸ—„ï¸ Persistent Conversation Management
- Thread-based conversation history
- PostgreSQL database for state persistence
- Run and step tracking for audit trails

### ğŸ“ˆ Evaluation Framework
- Dataset-based evaluation system
- Tool usage accuracy metrics
- Performance tracking across queries

---

## ğŸ› ï¸ Technology Stack

### Core Technologies
- **Python 3.x** - Primary programming language
- **LangChain** - LLM orchestration framework
- **LangGraph** - State machine and workflow management
- **FastAPI** - REST API framework
- **PostgreSQL** - Relational database for persistence
- **SQLAlchemy** - ORM for database operations

### AI/ML Components
- **OpenAI GPT** - Large Language Model (configurable)
- **Structured Output** - Pydantic models for type-safe LLM responses
- **Prompt Engineering** - Context-specific system prompts

### Data Processing
- **Pandas** - Data manipulation and analysis
- **NumPy** - Numerical computations
- **Pickle** - Serialized data storage

### Development Tools
- **Uvicorn** - ASGI server
- **python-dotenv** - Environment configuration
- **Pydantic** - Data validation

---

## ğŸ“ Project Structure

```
Thesis-AI/
â”œâ”€â”€ API/                          # REST API layer
â”‚   â”œâ”€â”€ api.py                   # Main API endpoints
â”‚   â””â”€â”€ SystemAPI.py             # System-level endpoints
â”‚
â”œâ”€â”€ Assistant/                    # Core agent implementation
â”‚   â”œâ”€â”€ ARDI.py                  # Main agent class
â”‚   â”œâ”€â”€ ARDIChat.py              # Chat interface wrapper
â”‚   â””â”€â”€ agent_core/              # Agent workflow components
â”‚       â”œâ”€â”€ planner.py           # Task planning logic
â”‚       â”œâ”€â”€ executor.py          # Task execution engine
â”‚       â”œâ”€â”€ responder.py         # Response generation
â”‚       â””â”€â”€ workflow.py          # LangGraph workflow
â”‚
â”œâ”€â”€ config/                       # Configuration files
â”‚   â”œâ”€â”€ settings.yaml            # LLM and system settings
â”‚   â””â”€â”€ prompts/                 # System prompts
â”‚       â”œâ”€â”€ 0.business_context.txt
â”‚       â”œâ”€â”€ 1.data_sources_context.txt
â”‚       â”œâ”€â”€ 2.tools_planning.txt
â”‚       â”œâ”€â”€ 2.plan_update.txt
â”‚       â”œâ”€â”€ 3.direct_response.txt
â”‚       â””â”€â”€ 4.response_stage.txt
â”‚
â”œâ”€â”€ crud/                         # Database operations
â”‚   â”œâ”€â”€ login.py
â”‚   â”œâ”€â”€ message.py
â”‚   â”œâ”€â”€ run.py
â”‚   â”œâ”€â”€ step.py
â”‚   â”œâ”€â”€ thread.py
â”‚   â”œâ”€â”€ tool.py
â”‚   â””â”€â”€ users.py
â”‚
â”œâ”€â”€ db/                           # Database setup
â”‚   â”œâ”€â”€ base.py                  # Base models
â”‚   â”œâ”€â”€ create_db.py             # Database initialization
â”‚   â”œâ”€â”€ insert_dataset.py        # Dataset insertion
â”‚   â””â”€â”€ session.py               # Session management
â”‚
â”œâ”€â”€ models/                       # SQLAlchemy models
â”‚   â”œâ”€â”€ datasetEvaluation.py
â”‚   â”œâ”€â”€ message.py
â”‚   â”œâ”€â”€ run.py
â”‚   â”œâ”€â”€ step.py
â”‚   â”œâ”€â”€ thread.py
â”‚   â”œâ”€â”€ toolCall.py
â”‚   â””â”€â”€ user.py
â”‚
â”œâ”€â”€ utils/                        # Utility modules
â”‚   â”œâ”€â”€ tools.py                 # Analytical tools implementation
â”‚   â””â”€â”€ utils.py                 # Helper functions
â”‚
â”œâ”€â”€ data/                         # Data sources
â”‚   â”œâ”€â”€ user_segments_viz.pkl    # User segmentation data
â”‚   â”œâ”€â”€ news_topics.pkl          # Topic modeling data
â”‚   â””â”€â”€ news_viz2.json           # Raw article data
â”‚
â”œâ”€â”€ datasets/                     # Evaluation datasets
â”‚   â””â”€â”€ evaluation_dataset.json
â”‚
â”œâ”€â”€ logs/                         # Application logs
â”‚   â””â”€â”€ system.log
â”‚
â”œâ”€â”€ playground.ipynb              # Development notebook
â”œâ”€â”€ ToolsDev.ipynb               # Tools development notebook
â”œâ”€â”€ .env                         # Environment variables
â””â”€â”€ README.md                    # This file
```

---

## ğŸš€ Installation

### Prerequisites
- Python 3.8 or higher
- PostgreSQL 12 or higher
- OpenAI API key (or compatible LLM provider)

### Step 1: Clone the Repository
```bash
git clone https://github.com/Joancf1997/Thesis-AI.git
cd Thesis-AI
```

### Step 2: Create Virtual Environment
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

### Step 3: Install Dependencies
```bash
pip install -r requirements.txt
```

*Note: If `requirements.txt` is not present, install the following packages:*
```bash
pip install fastapi uvicorn sqlalchemy psycopg2-binary python-dotenv \
            langchain langchain-openai langgraph pandas numpy pydantic
```

### Step 4: Set Up PostgreSQL Database
```bash
# Create database
createdb news_AI

# Or using psql
psql -U postgres
CREATE DATABASE news_AI;
\q
```

### Step 5: Configure Environment Variables
Create a `.env` file in the project root:
```env
PROJECT_NAME=ARDI AI Assistant
API_V1_STR=/api/v1
POSTGRES_SERVER=localhost
POSTGRES_PORT=5432
POSTGRES_USER=your_username
POSTGRES_PASSWORD=your_password
POSTGRES_DB=news_AI
DATABASE_URL=postgresql://your_username:your_password@localhost/news_AI
OPENAI_API_KEY=your_openai_api_key
```

### Step 6: Initialize Database
```bash
python db/create_db.py
```

### Step 7: Load Evaluation Dataset (Optional)
```bash
python db/insert_dataset.py
```

---

## âš™ï¸ Configuration

### LLM Configuration
Edit `config/settings.yaml` to configure the language model:

```yaml
llm:
  provider: openai          # Options: openai, ollama
  model_name: gpt-4         # Model identifier
  temperature: 0            # 0-1, controls randomness
  max_tokens: 2048          # Maximum response length
```

### Prompt Engineering
System prompts are located in `config/prompts/`:
- `0.business_context.txt` - Business domain and role definition
- `1.data_sources_context.txt` - Available data sources description
- `2.tools_planning.txt` - Tool descriptions for planning
- `2.plan_update.txt` - Dynamic plan update instructions
- `3.direct_response.txt` - Direct response (no tools) template
- `4.response_stage.txt` - Final response generation template

---

## ğŸ’» Usage

### Starting the API Server
```bash
uvicorn API.api:app --reload --root-path /API
```

#### 1. Create a User
```bash
curl -X POST "http://localhost:8000/users/create" \
  -H "Content-Type: application/json" \
  -d '{"username": "editor1", "password": "secure_password"}'
```

#### 2. Login
```bash
curl -X POST "http://localhost:8000/auth/login" \
  -H "Content-Type: application/json" \
  -d '{"username": "editor1", "password": "secure_password"}'
```

#### 3. Create a Conversation Thread
```bash
curl -X POST "http://localhost:8000/chat/newThread" \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "your-user-uuid",
    "name": "Segment Analysis Session"
  }'
```

#### 4. Ask a Question
```bash
curl -X POST "http://localhost:8000/chat/ask" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "Which regions are most engaged for the political debates segment?",
    "thread_id": "your-thread-uuid"
  }'
```

### Example Questions

**User Segment Analysis:**
- "Describe the reading behavior of segment 5"
- "Which regions are most active for the political debates segment?"
- "What time of day do readers in the sports segment engage most?"

**Topic Analysis:**
- "What are the most common topic transitions for segment 3?"
- "Predict the next likely topic after 'Politik' for segment 7"

**Article Performance:**
- "Show me the top articles read between 8am and 10am by segment 2"
- "Which articles have the highest engagement in the morning?"

**Temporal Patterns:**
- "When are readers most active during the day for segment 4?"
- "Compare morning vs evening reading patterns"

---

## ğŸ”Œ API Endpoints

### Authentication
- `POST /auth/login` - User authentication
- `POST /users/create` - Create new user
- `GET /users` - List all users

### Chat Interface
- `POST /chat/newThread` - Create new conversation thread
- `POST /chat/ask` - Submit question to agent
- `GET /chat/history/{thread_id}` - Retrieve conversation history
- `PUT /chat/thread/{thread_id}/rename` - Rename thread
- `DELETE /chat/thread/{thread_id}` - Delete thread

### Thread Management
- `GET /threads?user_id={uuid}` - Get user's threads

### System Endpoints
- `GET /UserSegments` - List all user segments
- `GET /UserSegment/{id}` - Get segment details

### Evaluation
- `POST /dataset_evaluation` - Run evaluation on dataset
- `GET /dataset_evaluations` - Retrieve evaluation results

---

## ğŸ”„ Agent Workflow

ARDI follows a multi-stage workflow implemented as a state machine:

### 1. Task Planning
```
User Question â†’ LLM Planning â†’ Structured Plan (JSON)
```
- Analyzes user intent
- Identifies required tools
- Creates dependency graph
- Outputs structured task list

### 2. Plan Validation
```
Structured Plan â†’ Validation â†’ [Valid] â†’ Execution
                             â†’ [Invalid] â†’ Replan
```
- Checks task IDs uniqueness
- Validates tool existence
- Verifies dependency integrity
- Ensures proper argument types

### 3. Plan Execution
```
Task 1 â†’ Task 2 â†’ ... â†’ Task N
  â†“        â†“              â†“
Output 1  Output 2    Output N
```
- Resolves dependencies
- Executes tools sequentially
- Manages data flow between tasks
- Supports dynamic replanning

### 4. Response Generation
```
Execution Outputs â†’ LLM Synthesis â†’ Natural Language Response
```
- Synthesizes results
- Generates insights
- Formats for editorial context
- Provides actionable recommendations

### State Machine Diagram
```
[START] â†’ [task_planning] â†’ [validate_plan] â†’ {validation_router}
                                                      â†“
                                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                    â†“                 â†“                 â†“
                            [direct_response]   [run_plan]      [task_planning]
                                    â†“                 â†“                 â†‘
                                  [END]    [generate_response]         â”‚
                                                      â†“                 â”‚
                                                    [END]               â”‚
                                                                        â”‚
                                    (invalid plan) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Data Sources

### 1. User Segmentation Data (`user_segments_viz.pkl`)
Behavioral clusters of news readers including:
- Segment descriptions and titles
- User type distribution (frequent/non-frequent)
- Regional consumption patterns
- Engagement metrics (scroll depth, time, words per minute)
- Topic transition probabilities
- Representative articles

### 2. News Topics Data (`news_topics.pkl`)
Topic modeling results including:
- Topic titles and descriptions
- High/low representative documents
- Topic clusters

### 3. Raw Articles Data (`news_viz2.json`)
Article metadata including:
- Article IDs and titles
- Teaser text
- Publication dates
- Topic clusters
- Engagement metrics

---

## ğŸ“ˆ Evaluation

### Evaluation Framework
The system includes an evaluation framework to assess agent performance:

#### Metrics
- **Tool Selection Accuracy**: Ratio of correctly selected tools
- **Task Completion Rate**: Percentage of successfully completed queries
- **Response Quality**: Human evaluation of generated insights

#### Running Evaluation
```bash
curl -X POST "http://localhost:8000/dataset_evaluation" \
  -H "Content-Type: application/json" \
  -d '{
    "user_id": "your-user-uuid",
    "name": "Evaluation Run 1"
  }'
```

#### Viewing Results
```bash
curl -X GET "http://localhost:8000/dataset_evaluations"
```

### Evaluation Dataset
Located in `datasets/evaluation_dataset.json`, containing:
- User queries
- Expected tool selections
- Ground truth answers

---

## ğŸ§ª Development

### Jupyter Notebooks
- `playground.ipynb` - Interactive development and testing
- `ToolsDev.ipynb` - Tool development and validation

### Adding New Tools
1. Implement function in `utils/tools.py`
2. Add to `TASK_FUNCS` dictionary
3. Update `config/prompts/2.tools_planning.txt` with tool description
4. Add argument type mapping in `executor.py` (if needed)

### Database Schema
The system tracks:
- **Users** - Authentication and ownership
- **Threads** - Conversation sessions
- **Messages** - User and assistant messages
- **Runs** - Agent execution instances
- **Steps** - Individual workflow stages
- **ToolCalls** - Tool invocations with inputs/outputs
- **DatasetEvaluations** - Evaluation results

---

## ğŸ¤ Contributing

This is a thesis project, but contributions and suggestions are welcome:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/improvement`)
3. Commit changes (`git commit -am 'Add new feature'`)
4. Push to branch (`git push origin feature/improvement`)
5. Create Pull Request

---

## ğŸ“ License

This project is part of a Master's thesis at the Free University of Bozen-Bolzano.  
Please contact the author for licensing information.

---

## ğŸ‘¤ Author

**Jose Andres**  
Master's Student in Data Science  
Free University of Bozen-Bolzano (UNIBZ)

---

## ğŸ™ Acknowledgments

- Free University of Bozen-Bolzano for academic support
- Davisd Massimo for the guidance and support during the thesis
- News organization for providing anonymized data

---

## ğŸ“š References

- [LangChain Documentation](https://python.langchain.com/)
- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [FastAPI Documentation](https://fastapi.tiangolo.com/)
- [OpenAI API Documentation](https://platform.openai.com/docs/)

---

**Last Updated**: January 2026
